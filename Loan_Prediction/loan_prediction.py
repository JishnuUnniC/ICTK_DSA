# -*- coding: utf-8 -*-
"""Loan_Prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-RssBWbkNgiGfVllZn2_nqppPZfj--re

# Libraries
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.preprocessing import LabelEncoder,OrdinalEncoder
from sklearn.preprocessing import OneHotEncoder

from imblearn.over_sampling import RandomOverSampler
from imblearn.under_sampling import RandomUnderSampler

# from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix

train_file_path = 'train_ctrUa4K.csv'
test_file_path = 'test_lAUu6dG.csv'

df_train = pd.read_csv(train_file_path)
df_test = pd.read_csv(test_file_path)

df_train.head()

df_test.head()

df_train.info()

df_test.info()

df_train.describe()

df_test.describe()

"""# Preprocessing"""

df_train.duplicated().sum()

df_train.duplicated().sum()

# missing values in train data
df_train.isnull().sum()

# LoanAmount missing-> remove row. ( percentage of missing data is 5.2)
# Loan_Amount_Term missing-> remove row. ( percentage of missing data is 3.58)
# Credit_History missing -> remove row. ( percentage of missing data is 8.15)
# we need to recheck the other missing values after we handle these 3 critical information.
#

label_enc= LabelEncoder()
df_train['Dependents'] = label_enc.fit_transform(df_train['Dependents'])

df_train.dropna(subset=['LoanAmount','Loan_Amount_Term','Credit_History'],inplace=True)
#df_train.dropna(inplace= True)
df_train.isnull().sum()
# It is ok to replace the missing values with mode of each columns.

df_train['Gender'].fillna(df_train['Gender'].mode()[0],inplace=True)
df_train['Married'].fillna(df_train['Married'].mode()[0],inplace=True)
df_train['Dependents'].fillna(df_train['Dependents'].median(),inplace=True)
df_train['Self_Employed'].fillna(df_train['Self_Employed'].mode()[0],inplace=True)
df_train.isnull().sum()

# missing values in train data
df_test.isnull().sum()

# since the test environment need to maintain the shape, we are not dropping any rows

label_enc= LabelEncoder()
df_test['Dependents'] = label_enc.fit_transform(df_test['Dependents'])

# since the count of test values are relatively high, we can afford to drop all the null values.
# but, since the sample output has all rows, we should not drop any value but, replace missing values.
df_test['Gender'].fillna(df_train['Gender'].mode()[0],inplace=True)
df_test['Dependents'].fillna(df_train['Dependents'].median(),inplace=True)
df_test['Self_Employed'].fillna(df_train['Self_Employed'].mode()[0],inplace=True)
df_test['Credit_History'].fillna(df_train['Credit_History'].mode()[0],inplace=True)

df_test['LoanAmount'].fillna(df_train['LoanAmount'].mean(),inplace=True)

df_test['Loan_Amount_Term'].fillna(df_train['Loan_Amount_Term'].median(),inplace=True)

df_test.isnull().sum()

df_train['Loan_ID'].duplicated().sum()

df_test['Loan_ID'].duplicated().sum()

# since LoanID is not a contributing factor to the prediction of loan approval, we can drop that column.

df_test_loanID = df_test['Loan_ID']

df_train.drop('Loan_ID', inplace=True, axis=1)
df_test.drop('Loan_ID', inplace=True, axis=1)

df_train.isna().sum()

# bina_order = ['Yes', 'No']
# status_order = ['Y','N']
gender_order = ['Male', 'Female']
property_order = ['Urban', 'Semiurban', 'Rural']


ordinal_enc = OrdinalEncoder(categories=[gender_order])
df_train['Gender']=ordinal_enc.fit_transform(df_train[['Gender']])
df_test['Gender']=ordinal_enc.fit_transform(df_test[['Gender']])

ordinal_enc = OrdinalEncoder(categories=[property_order])
df_train['Property_Area']=ordinal_enc.fit_transform(df_train[['Property_Area']])
df_test['Property_Area']=ordinal_enc.fit_transform(df_test[['Property_Area']])

label_enc = LabelEncoder()

for col in ['Married', 'Self_Employed', 'Education', 'Dependents', 'Loan_Status']:
    df_train[col] = label_enc.fit_transform(df_train[col])

for col in ['Married', 'Self_Employed', 'Education', 'Dependents']:
    df_test[col] = label_enc.fit_transform(df_test[col])

#df_train['Married', 'Self_Employed', 'Education', 'Dependents', 'Loan_Status']= label_enc.fit_transform(df_train[['Married', 'Self_Employed', 'Education', 'Dependents', 'Loan_Status']])
#df_test['Married', 'Self_Employed', 'Education', 'Dependents']= label_enc.fit_transform(df_test[['Married', 'Self_Employed', 'Education', 'Dependents']])

df_train.head(3)

df_test.head(3)

sns.heatmap(df_train.corr(), annot=True, fmt=".1f")
# no need to drop any columns.

sns.heatmap(df_test.corr(), annot=True, fmt=".1f")
# no need to drop any columns.

def plot_box_for_df( datafrm, rows, cols ):     # function written to plot boxplots as subplots in a generic way.
  plot_position = 1     # variable introduced to make positioning of the plot easier.
  for col in datafrm.columns:
    plt.subplot( rows, cols, plot_position )
    datafrm[[col]].boxplot()
    plt.xticks(rotation=0)
    plt.title(col+' Box')
    plot_position += 1  # increments plot_position to fix the plot position

  plt.tight_layout()        # makes sure that the plots do not overlap each other.
  plt.show()

df_num_train = df_train[['Dependents', 'ApplicantIncome', 'CoapplicantIncome', 'LoanAmount']]
df_num_test = df_test[['Dependents', 'ApplicantIncome', 'CoapplicantIncome', 'LoanAmount']]

plot_box_for_df(df_num_train, 2, 2)
# since all the 3 values ('ApplicantIncome', 'CoapplicantIncome', 'LoanAmount') are highly significant,
# we are not clipping the outliers. so that, we get the proper relevance to the loan_status.

plot_box_for_df(df_num_test, 2, 2)
# since all the 3 values ('ApplicantIncome', 'CoapplicantIncome', 'LoanAmount') are highly significant,
# we are not clipping the outliers. so that, we get the proper relevance to the loan_status.

# Function to plot bar graphs in a subplot.
def plot_bar_for_df(datafrm, rows, cols, graph_type):
  plt.figure(figsize = (9,9))
  plot_position = 1     # variable introduced to make positioning of the plot easier.
  for col in datafrm.columns:
    plt.subplot(rows, cols, plot_position)
    datafrm[col].value_counts().sort_index().plot(kind=graph_type)  # here, sorting is done based on count.
    #datafrm[col].value_counts().plot(kind=graph_type)               # here, sorting is done based on value.
    plt.xticks(rotation=0)
    plt.title(col+' Distribution')
    plot_position += 1  # increments plot_position to fix the plot position

  plt.tight_layout()        # makes sure that the plots do not overlap each other.
  plt.show()

plot_bar_for_df(df_train, 4, 3, 'bar')

plot_bar_for_df(df_test, 4, 3, 'bar')

# applying min max scaler for 'ApplicantIncome', 'CoapplicantIncome', 'LoanAmount'
# apply standardScaler for Loan_Amount_Term.

scaler = MinMaxScaler()
df_train[['ApplicantIncome', 'CoapplicantIncome']] = scaler.fit_transform(df_train[['ApplicantIncome', 'CoapplicantIncome']])
df_test[['ApplicantIncome', 'CoapplicantIncome']] = scaler.fit_transform(df_test[['ApplicantIncome', 'CoapplicantIncome']])

scaler = StandardScaler()

df_train[['LoanAmount']] = scaler.fit_transform(df_train[['LoanAmount']])
df_test[['LoanAmount']] = scaler.fit_transform(df_test[['LoanAmount']])

df_train[['Loan_Amount_Term']] = scaler.fit_transform(df_train[['Loan_Amount_Term']])
df_test[['Loan_Amount_Term']] = scaler.fit_transform(df_test[['Loan_Amount_Term']])

df_train.head()

df_test.head()

def remove_outliers_zscore(datafrm, col, threshold=2):
    # Check if the specified columns exist in the DataFrame
    if col not in datafrm.columns:
      raise ValueError(f"Column '{col}' not found in the DataFrame.")

    # Calculate the Z-scores for each column
    z_scores = np.abs(datafrm[col])

    # Identify rows where Z-score is less than the threshold for all columns
    #condition = (z_scores < threshold).all(axis=1)
    filtered_data = datafrm[z_scores < threshold] # if z_score < threshold, keep that row

    # Filter out rows where any column exceeds the Z-score threshold
    datafrm = filtered_data

    #return datafrm

df_train.shape

remove_outliers_zscore(df_train, 'LoanAmount')
remove_outliers_zscore(df_train, 'Loan_Amount_Term')

df_train.shape

df_train.head(5)

df_train['Loan_Status'].value_counts()

# there is a disparity in distribution of data in target column ( i.e., Loan_Status)
# to avoid data imbalance, we need to oversample the column data.

x= df_train.drop('Loan_Status', axis=1)
y= df_train['Loan_Status']

oversampler = RandomOverSampler(random_state=42)
x_resampled, y_resampled = oversampler.fit_resample(x, y)

balanced_df_test= pd.DataFrame(x_resampled, columns=x.columns)
balanced_df_test['Loan_Status'] = y_resampled

balanced_df_test['Loan_Status'].value_counts()

"""# Model training"""

X_train = df_train.drop('Loan_Status', axis=1)
y_train = df_train['Loan_Status']

X_test = df_test

"""## PCA"""

from sklearn.decomposition import PCA
# doing dimensionality reduction to simplify the
pca = PCA(n_components=8)
X_train = pca.fit_transform(X_train)
X_test = pca.transform(X_test)

explained_variance=pca.explained_variance_ratio_
np.cumsum(explained_variance)

"""## KNN"""

knn = KNeighborsClassifier(n_neighbors=3)
knn.fit(X_train, y_train)

y_pred_KNN = knn.predict(X_test) # Predicted classification using knn.

"""## Logistic Regression"""

log_reg_model = LogisticRegression()
log_reg_model.fit(X_train, y_train)

y_pred_LR = log_reg_model.predict(X_test) # Predicted classification using logistic regression.

"""## Naive Bayes"""

naive_bayes = GaussianNB()
naive_bayes.fit(X_train, y_train)

y_pred_NB = naive_bayes.predict(X_test)

"""## SVM"""

svm_clssfctn = SVC(kernel='linear')
svm_clssfctn.fit(X_train, y_train)

y_pred_SVM = svm_clssfctn.predict(X_test)

"""## Decision Tree"""

dcsn_tree = DecisionTreeClassifier()
dcsn_tree.fit(X_train, y_train)

y_pred_DT = dcsn_tree.predict(X_test)

"""## Random Forest"""

random_forest = RandomForestClassifier()
random_forest.fit(X_train, y_train)

y_pred_RF = random_forest.predict(X_test)

"""# Output file creation"""

def prep_output (X_test_loanID, y_pred, algo_type ):
  df_pred = pd.DataFrame(y_pred, columns = ['Loan_Status'])
  df_pred['Loan_Status'] = df_pred['Loan_Status'].replace({0: 'N', 1: 'Y'})

  df_op= pd.concat([X_test_loanID, df_pred], axis = 1, ignore_index=True)
  df_op.rename(columns={0: 'Loan_ID', 1: 'Loan_Status'}, inplace=True)
  file_path = algo_type + '_output.csv'
  df_op.to_csv(file_path, index=False)
  return df_op

# Displaying classification algorithm results.
df_op_LR = prep_output(df_test_loanID, y_pred_LR, 'Logistic_Regression')
df_op_KNN = prep_output(df_test_loanID, y_pred_KNN, 'KNN')
df_op_NB = prep_output(df_test_loanID, y_pred_NB, 'Naive_Bayes')
df_op_SVM = prep_output(df_test_loanID, y_pred_SVM, 'SVM')
df_op_DT = prep_output(df_test_loanID, y_pred_DT, 'Decision_Tree')
df_op_RF = prep_output(df_test_loanID, y_pred_RF, 'Random_Forest')

df_op_LR['Loan_Status'].value_counts()

df_op_KNN['Loan_Status'].value_counts()

df_op_NB['Loan_Status'].value_counts()

df_op_SVM['Loan_Status'].value_counts()

df_op_DT['Loan_Status'].value_counts()

df_op_RF['Loan_Status'].value_counts()